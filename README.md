# 🌊 简易流式计算系统 - 数据小溪工厂 🏭

## 🌟 简介：像搭积木一样玩转大数据！

欢迎来到"数据小溪工厂"！这是一个超迷你、超好玩的流式计算系统，专为想入门大数据处理的小伙伴量身定制！想象一下，数据就像小溪里的水，源源不断地流过来。我们的系统就像一个神奇的水处理工厂，把这些"数据小溪"加工成我们需要的"产品"！

**核心思想：**  数据不停流，计算不停止！就像工厂的流水线，数据一来就开始处理，结果立刻出来！

**核心组件：**  我们工厂有几个关键的车间：

- **进水口 (Source Operator):**  工厂的入口，负责从外面"接水"进来。我们用 **KafkaSourceOperator** 从 Kafka 这个"大水管"里接数据。
- **加工车间 (Operator):**  水接进来要处理呀！这里有很多不同的"工人" (Operator)，每个工人负责一道工序：
    - **Map Operator (地图工人):**  负责"变形"数据，比如把脏水变清水！
    - **KeyBy Operator (分拣工人):**  负责"分类"数据，比如把清水按用途分成几堆！
    - **Reduce Operator (聚合工人):**  负责"统计"数据，比如数数每堆水有多少！
- **出水口 (Sink Operator):**  水处理好了，要"排出去"！**ConsoleSinkOperator** 直接把结果打印到屏幕上给你看，简单直接！
- **生产线 (Pipeline):**  把所有车间串起来的"传送带"！**Pipeline** 负责把进水口、加工车间、出水口连成一条线，让数据像流水一样，从头走到尾，自动处理！

## ✨ 功能特性 ( - 麻雀虽小，五脏俱全！)

- **核心算子**:  我们提供了几个"明星工人"：`KafkaSourceOperator` (接水工), `MapOperator` (地图工), `KeyByOperator` (分拣工), `ReduceOperator` (聚合工), `ConsoleSinkOperator` (喇叭花 - 喊出结果),  **以及新增的 `FileSinkOperator` (文件记录员 - 把结果写到文件里)**!
- **数据包 (DataPacket)**:  数据在工厂里流动的小包裹，每个包裹都装着数据和一些"标签"（时间戳、水位线等）。
- **算子链 (Pipeline)**:  用 `Pipeline` 把算子像搭积木一样连接起来，形成数据处理的流水线！
- **窗口机制 (Window)**:  `ReduceOperator` 里实现了"时间窗口"，就像设定一个时间段，只统计这段时间里的数据。
- **状态管理 (StateBackend)**:  用 `MemoryStateBackend` 实现了简单的"记忆"功能，让工人记住之前处理过的数据。
- **控制台输出 (ConsoleSinkOperator)**:  结果直接打印在控制台上，眼见为实！
- **文件输出 (FileSinkOperator)**:  **新增功能！** 可以把处理结果保存到文件里，方便后续查看和分析！

## 🛠️ 环境配置 (开始搭建你的数据工厂！)

### 前置要求 (你需要先准备这些工具)

- **Python 3.8+**:  工厂的"语言"，确保你的电脑上安装了 Python 3.8 或更高版本。
- **Docker & Docker Compose**:  用来快速搭建 Kafka "水管"，不用你手动安装，省时省力！
- **pip**:  Python 的"工具箱"，用来安装项目需要的各种"工具"（依赖包）。

### 安装步骤 (跟着步骤一步步搭建)

1. **克隆项目仓库 (把工厂图纸下载下来)**

   ```bash
   git clone <project_url>  # 把你的项目仓库地址替换到这里
   cd <project_directory>   # 进入项目目录
   ```

2. **安装 Python 依赖 (给工人配备工具)**

   ```bash
   python -m pip install -r requirements.txt
   ```
   或者，更简单的方式，运行我们提供的安装脚本：
   ```bash
   python scripts/setup.py install
   ```

3. **启动 Kafka 环境 (启动数据源 "水管")**

   - **Linux/macOS**: 运行 `scripts/start_kafka.sh`
   - **Windows**: 运行 `scripts\start_kafka.bat`

### 验证安装 (检查工厂是否正常运转)

运行测试数据生成器，向 Kafka `test_topic` 主题发送一些测试数据，看看是否一切正常：

```bash
python examples/data_generator.py
```

## 🚀 快速开始 - 单词计数示例 (让工厂跑起来，数数文章里的单词！)

1. **启动 Kafka 环境** (如果还没启动，先启动 "水管")
   ```bash
   # Windows用户
   scripts\start_kafka.bat

   # Linux/Mac用户
   ./scripts/start_kafka.sh
   ```


2. **运行单词计数示例** (消费者程序！)（这个顺序很重要！）
   ```bash
   python examples/word_count.py
   ```
   这样消费者程序会先启动并连接到Kafka，准备好接收消息。。

3. **运行数据生成器** （生产者程序）
   ```bash
   python examples/data_generator.py
   ```
   数据生成器会开始向Kafka发送消息，而已经运行的消费者程序会立即处理这些消息

4. **查看控制台输出** (检查 "出水口"，看结果！)
   你应该能看到类似这样的输出，表示单词计数成功啦！
   ```
   ==================================================
   🔢 结果 #1 | ⏰ 时间戳: 1709574867984
   --------------------------------------------------
   📊 单词: '流式计算' | 出现次数: 2
   ==================================================
   ```
## 为什么这个顺序很重要？
这个顺序很重要是因为：
1.如果先启动数据生成器，它发送的一些早期消息可能会被错过，因为消费者还没有准备好接收
2.消费者程序需要时间初始化Pipeline和所有算子，确保处理链准备就绪
3.在流处理系统中，通常希望消费者先注册并准备好接收数据流

## ❓ 常见问题解决方案 (遇到问题别慌，这里有 "维修指南")

### 1. 没有看到输出结果？ (出水口没水？)

- **检查 Kafka 是否正常运行**:  用 `docker ps` 命令查看 Kafka 容器是否在运行，确保 "水管" 畅通。
- **检查数据生成器**:  确保 `data_generator.py` 正在运行，并且没有报错，看看 "水龙头" 是否打开。
- **检查主题名称**:  确保 `data_generator.py` 和 `word_count.py` 使用的是 **相同** 的 Kafka 主题名字，主题名字要对得上，数据才能正确流入。
- **检查日志输出**:  仔细查看控制台的日志输出，有没有错误信息？特别是注意有没有 `'int' object is not subscriptable` 这种错误。

### 2.  `'int' object is not subscriptable` 错误? (遇到 "数据格式不匹配" 的问题了！)

这个问题通常是因为 **Source 算子无法正确解析 Kafka 消息格式**。就像工厂的进水口，只认识特定格式的 "水"，如果来的 "水" 格式不对，就会报错。

**解决方案**:  我们已经在 `operators/source_operator.py` 中改进了消息处理逻辑，让 "进水口" 更智能，能处理更多格式的 "水"！**请确保你使用的是最新的代码！**

### 3. 如何自定义数据处理逻辑? (我想让工厂加工不同的 "产品"！)

很简单！只需修改 `examples/word_count.py` 中的 **映射函数 (map_function)**、**键选择器 (key_selector)** 和 **归约函数 (reduce_function)**，就可以实现各种不同的数据处理逻辑。就像更换工厂里的不同 "工人"，生产不同的 "产品"！

### 4. 配置中缺少必需的 name 参数

这个问题通常是因为 **Pipeline 配置中缺少必需的 name 参数**。就像工厂的 "生产线"，需要给每个生产线取个名字，才能正常工作。
同样地，你可能需要修改其他算子的初始化方式。确保所有算子都使用配置字典而不是直接参数：

### 5. 启动顺序错误
症状：程序启动但没有显示结果
解决：确保严格按照上面的启动顺序操作：先Kafka，再word_count，最后data_generator

### 6.问题：算子初始化失败
症状：终端显示"初始化算子失败"错误
解决：检查算子配置，特别是operator_id字段是否正确设置

### 7.问题：Kafka连接失败
症状：无法连接到Kafka
解决：确保Kafka服务已正常启动，并且bootstrap_servers设置为"localhost:9092"

## 📂 项目结构 (工厂的 "地图"，方便你找到各个 "车间")

```
stream-computing-system/
├── core/                      # 核心模块 (工厂的 "心脏")
│   ├── data_packet.py         # 数据包定义 (数据 "包裹" 的设计图纸)
│   ├── pipeline.py            # 数据处理管道 (生产线 "图纸")
│   └── state_backend.py       # 状态后端接口 ("记忆" 功能的接口)
├── operators/                 # 算子实现 (各种 "工人" 的具体代码)
│   ├── source_operator.py     # 数据源算子 ("进水口工人")
│   ├── map_operator.py        # 映射算子 ("地图工人")
│   ├── keyby_operator.py      # 分组算子 ("分拣工人")
│   ├── reduce_operator.py     # 归约算子 ("聚合工人")
│   └── sink_operator.py       # 输出算子 ("出水口工人")
├── examples/                  # 示例代码 (工厂的 "示例车间")
│   ├── data_generator.py      # 数据生成器 ("注水工具"生产者)
│   ├── word_count.py          # 单词计数示例 ("工厂启动脚本"消费者)
├── scripts/                   # 脚本 (各种 "工具")
│   ├── start_kafka.sh/bat     # Kafka 启动脚本 ("水龙头")
│   ├── setup.py               # 安装脚本 ("工具安装脚本")
├── docker-compose.yml         # Docker Compose 配置 (Kafka "水管" 的配置)
├── README.md                  # 项目说明文档 (就是你现在看到的这份 "说明书"!)
├── requirements.txt           # Python 依赖文件 ("工具清单")
```



记住：数据就是水流，算子就是水处理工人！🌊👨‍🔧

**更多信息，请参考代码注释和具体代码实现。**